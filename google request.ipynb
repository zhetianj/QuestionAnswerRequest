{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from contextlib import contextmanager\n",
    "import copy\n",
    "from functools import partial\n",
    "from itertools import chain\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ztjin/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/ztjin/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/ztjin/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/ztjin/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/ztjin/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/ztjin/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, Sampler, DataLoader\n",
    "from torch.optim.optimizer import Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_PATH = \"./input/crawl-300d-2M.vec\"\n",
    "TRAIN_PATH = \"./input/train.csv\"\n",
    "TEST_PATH = \"./input/test.csv\"\n",
    "SUBMISSION_PATH = \"./input/sample_submission.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 300\n",
    "max_len = 200\n",
    "max_words = 10000\n",
    "\n",
    "batch_size = 512\n",
    "train_epoch = 3\n",
    "n_split = 5\n",
    "\n",
    "miu = 0.9\n",
    "update_per_epoch = 10\n",
    "\n",
    "seed = 201912\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "lc = LancasterStemmer()\n",
    "sb = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer(msg):\n",
    "    t0 = time.time()\n",
    "    print('%s start.'%msg)\n",
    "    yield\n",
    "    elapsed_time = time.time() - t0\n",
    "    print('%s done in %s min.'%(msg, elapsed_time/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "misspell_dict = {\"aren't\": \"are not\", \"can't\": \"cannot\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_typical_mis(text):\n",
    "    mis_re = re.compile(\"(%s)\"%\"|\".join(misspell_dict.keys()))\n",
    "    return mis_re.sub(lambda x:misspell_dict[x.group(0)], text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']',\n",
    "          '>', '%', '=', '#', '*', '+', '\\\\', '•', '~', '@', '£', '·', '_', '{', '}', '©', '^',\n",
    "          '®', '`', '<', '→', '°', '€', '™', '›', '♥', '←', '×', '§', '″', '′', 'Â', '█',\n",
    "          '½', 'à', '…', '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶',\n",
    "          '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', '▒', '：', '¼',\n",
    "          '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲',\n",
    "          'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', '∙', '）', '↓', '、', '│', '（', '»', '，', '♪',\n",
    "          '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add space before & after puctuation \n",
    "def add_space(x):\n",
    "    x = str(x)\n",
    "    for punct in puncts + list(string.punctuation):\n",
    "        if punct in x:\n",
    "            x = x.replace(punct, ' ' + punct + ' ')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_numbers(x):\n",
    "    return re.sub(r'\\d+', ' ', x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding(embedding_path, word_index):\n",
    "    embedding_dict = dict((o.strip().split(' ')[0], o.strip().split(' ')[1:]) for o in open(embedding_path))\n",
    "    nb_words = min(2 + max_words, len(word_index))\n",
    "    embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "    \n",
    "    for word, index in word_index.items():\n",
    "        embedding_vec = embedding_dict.get(word)\n",
    "        if embedding_vec != None:\n",
    "            embedding_matrix[index] = embedding_vec\n",
    "            continue\n",
    "        #try upper & lower case\n",
    "        embedding_vec = embedding_dict.get(word.lower())\n",
    "        if embedding_vec != None:\n",
    "            embedding_matrix[index] = embedding_vec\n",
    "            continue\n",
    "        \n",
    "        embedding_vec = embedding_dict.get(word.upper())\n",
    "        if embedding_vec != None:\n",
    "            embedding_matrix[index] = embedding_vec\n",
    "            continue\n",
    "            \n",
    "        embedding_vec = embedding_dict.get(word.capitalize())\n",
    "        if embedding_vec != None:\n",
    "            embedding_matrix[index] = embedding_vec\n",
    "            continue\n",
    "        \n",
    "        embedding_vec = embedding_dict.get(ps.stem(word))\n",
    "        if embedding_vec != None:\n",
    "            embedding_matrix[index] = embedding_vec\n",
    "            continue\n",
    "        \n",
    "        embedding_vec = embedding_dict.get(lc.stem(word))\n",
    "        if embedding_vec != None:\n",
    "            embedding_matrix[i] = embedding_vec\n",
    "            continue\n",
    "            \n",
    "        embedding_vec = embedding_dict.get(sb.stem(word))\n",
    "        if embedding_vec != None:\n",
    "            embedding_matrix[i] = embedding_vec\n",
    "            continue\n",
    "        \n",
    "        return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_process():\n",
    "    train = pd.read_csv(TRAIN_PATH)\n",
    "    test = pd.read_csv(TEST_PATH)\n",
    "    \n",
    "    #process basic steps\n",
    "    process_cols = ['question_title', 'question_body', 'answer']\n",
    "    for col in process_cols:\n",
    "        train[col] = train[col].astype(str).apply(lambda x: x.lower()).apply(replace_typical_mis).apply(add_space).apply(clean_numbers).apply(lambda x: x.strip())\n",
    "        test[col] = test[col].astype(str).apply(lambda x: x.lower()).apply(replace_typical_mis).apply(add_space).apply(clean_numbers).apply(lambda x: x.strip())\n",
    "\n",
    "    #have no na value\n",
    "    #get targets\n",
    "    question_target_cols = train.columns[11:-9]\n",
    "    answer_target_cols = train.columns[-9:]\n",
    "    \n",
    "\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    train_index = np.random.permutation(len(train))\n",
    "    \n",
    "    train_x = train[process_cols][train_index]\n",
    "    train_question_y = train[question_target_cols][train_index]\n",
    "    train_answer_y = train[answer_target_cols][train_index]\n",
    "    \n",
    "    return train_x, train_question_y, train_answer_y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(texts, max_features = max_words):\n",
    "    #build token_to_id & id_to_token\n",
    "    counter = Counter()\n",
    "    for text in texts:\n",
    "        counter.update(text.split())\n",
    "\n",
    "    vocab = {\n",
    "        'token2id': {'<PAD>': 0, '<UNK>': max_features + 1, '<START>': max_features + 2},\n",
    "        'id2token': {}\n",
    "    }\n",
    "    vocab['token2id'].update(\n",
    "        {token: _id + 1 for _id, (token, count) in\n",
    "         enumerate(counter.most_common(max_features))})\n",
    "    vocab['id2token'] = {v: k for k, v in vocab['token2id'].items()}\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(texts, vocab):\n",
    "    #get the first 200 words of a text and transfer it to ids\n",
    "    return [[vocab['token2id'].get(token) for token in text.split()[:max_len]] for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
